{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juaIC0gSA8Oa"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/zecevic-matej/SSDS-Causal-ML/cff30a19806165ccaaf59904a9532ba2fc306a8e/Banner-Tutorial.png\" height=\"200\" />\n",
    "\n",
    "<h1>Hands-on: Causality for Machine Learning</h1>\n",
    "\n",
    "This tutorial combines elements from two previously existing tutorials. One of them first authored by [Alexandre Drouin](https://www.alexdrouin.com/) with contributions from [Philippe Brouillard](https://philippe-brouillard.com/) and [Thibaud Godon](https://github.com/thibgo)\n",
    "\n",
    "<br />\n",
    "\n",
    "**Abstract:** This tutorial will consist of a practical introduction to the estimation of causal effects. We will experiment with the concepts of average treatment effect, randomization, covariate adjustment, and inverse probability weighting to derive common estimators from the literature. We will also see where machine learning models fit into such estimators. Formal derivations will be presented and supported by extensive visualizations.\n",
    "\n",
    "<br />\n",
    "\n",
    "**Outline**\n",
    " * Exercise 1: Simpson's paradox (15 min)\n",
    " * Exercise 2: Identification and estimation via parent adjustment (20 min)\n",
    " * Exercise 3: From parent to back-door adjustment (20 min)\n",
    " * Exercise 4: Estimation via machine learning (15 min)\n",
    " * Exercise 5: Causal Structure Learning (10 min)\n",
    "\n",
    "<br />\n",
    "\n",
    "**Before you start**\n",
    " * Enable GPU in your environment in the menu via `Runtime -> Change runtime type` and setting the \"hardware accelerator\" to GPU. If it fails due to GPUs being unavailable, it's ok. Exercise 5 will just be longer to run.\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    " **Notes about the math:**\n",
    " \n",
    " * When writing probabilistic expressions, we will. use $P(a, b, c)$ instead of $P(A = a, B = b, C = c)$, unless it is required to disambiguate something.\n",
    " * In the equations, we assume discrete probabilities, but everything that we present can be generalized to the continuous case.\n",
    " * The presentation will focus on average treatment effects (ATE), but all the equations that we present can be generalized to include conditioning to estimate the conditional ATE (CATE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-g2nW5-lU77"
   },
   "source": [
    "## Utility functions\n",
    "\n",
    "Run these functions to bootstrap the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQSPHKaiJhmJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from google.colab import data_table\n",
    "data_table.enable_dataframe_formatter()\n",
    "\n",
    "%pip install igraph\n",
    "%pip install causal-learn\n",
    "import random\n",
    "import utils\n",
    "from string import ascii_uppercase\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from linear import notears_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0fRLnCOhbqu"
   },
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def pprint_ates(ate):\n",
    "    \"\"\"\n",
    "    Pretty print average treatment effect table\n",
    "    \n",
    "    \"\"\"\n",
    "    return HTML(pd.DataFrame([dict(method=k, ATE=v) for k, v in ate.items()]).to_html(index=False))\n",
    "\n",
    "\n",
    "def generate_data_ex1(n_samples=10000, seed=42):\n",
    "    random = np.random.RandomState(seed)\n",
    "\n",
    "    Z = random.randint(0, 3, size=n_samples)\n",
    "    \n",
    "    policy = [0.05, 0.5, 0.9]  # Probability of treatment for each Z\n",
    "    A = np.array([random.rand() <= policy[z] for z in Z]).astype(int)\n",
    "\n",
    "    # Value of Z: 0     1    2\n",
    "    survival = [[0.75, 0.25, 0.1],  # Untreated (A = 0)\n",
    "                [0.95, 0.75, 0.25]]  # Treated (A = 1)\n",
    "    Y = np.fromiter((random.rand() <= survival[A[i]][Z[i]] for i in range(n_samples)), dtype=int)\n",
    "\n",
    "    return pd.DataFrame(dict(Z=Z, A=A, Y=Y))\n",
    "\n",
    "\n",
    "def image_base64(img_array):\n",
    "    \"\"\"\n",
    "    Convert an image from numpy array to bytes\n",
    "    \n",
    "    Adapted from https://www.kaggle.com/code/stassl/displaying-inline-images-in-pandas-dataframe/notebook\n",
    "\n",
    "    \"\"\"\n",
    "    with BytesIO() as buffer:\n",
    "        im = Image.fromarray(img_array)\n",
    "        im.thumbnail((200, 200), Image.LANCZOS)\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "\n",
    "def image_formatter(im):\n",
    "    \"\"\"\n",
    "    Format image to show inline in dataframe\n",
    "\n",
    "    Adapted from https://www.kaggle.com/code/stassl/displaying-inline-images-in-pandas-dataframe/notebook\n",
    "\n",
    "    \"\"\"\n",
    "    return f'<img src=\"data:image/jpeg;base64,{im}\" width=\"100\">'\n",
    "\n",
    "\n",
    "def generate_data_ex4b(randomize=False, seed=42):\n",
    "    \"\"\"\n",
    "    Generate a dataset where confounding arises from a high-dimensional visual signal.\n",
    "\n",
    "    We use images of cats and dogs from CIFAR10 to serve as confounder.\n",
    "\n",
    "    \"\"\"\n",
    "    random = np.random.RandomState(seed)\n",
    "\n",
    "    # Load image dataset\n",
    "    base_dataset = CIFAR10(\".\", download=True)\n",
    "\n",
    "    # Normalization applied to images\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Extract the list of indices for all individuals with class cat (3) and dog (5)\n",
    "    class1_idx = np.where(np.array(base_dataset.targets) == 3)[0].tolist()\n",
    "    class2_idx = np.where(np.array(base_dataset.targets) == 5)[0].tolist()\n",
    "\n",
    "    # Keep only a few images and make them occur more than once\n",
    "    # By doing this, we aim to create profiles of individuals\n",
    "    # with the same visual profile (related to stratification)\n",
    "    class1_idx = class1_idx[: 100] * 100\n",
    "    class2_idx = class2_idx[: 100] * 100\n",
    "    n_samples = len(class1_idx) + len(class2_idx)\n",
    "\n",
    "    # Retrieve images from dataset and apply transformations\n",
    "    Z = np.array(base_dataset.data[class1_idx + class2_idx])\n",
    "    Z_normed = np.array([transform(z).numpy() for z in Z])\n",
    "\n",
    "    # We assume that treatment was assigned based on perfect knowledge of the classes.\n",
    "    Z_ = np.array([0] * len(class1_idx) + [1] * len(class2_idx))\n",
    "\n",
    "    if randomize:\n",
    "        policy = [0.5, 0.5]\n",
    "    else:\n",
    "        policy = [0.3, 0.7]  # Probability of treatment for each Z\n",
    "    A = np.array([random.rand() <= policy[z] for z in Z_]).astype(int)\n",
    "\n",
    "    # Value of Z:  0     1\n",
    "    survival = [[0.75, 0.10],  # Untreated (A = 0)\n",
    "                [0.95, 0.25]]  # Treated (A = 1)    \n",
    "    Y = np.fromiter((random.rand() <= survival[A[i]][Z_[i]] for i in range(n_samples)), dtype=int)\n",
    "\n",
    "    # Create a random permutation for the examples\n",
    "    shuffler = np.arange(n_samples)\n",
    "    random.shuffle(shuffler)\n",
    "\n",
    "    # Assemble final dataset\n",
    "    return pd.DataFrame(dict(Z=[z for z in Z_normed[shuffler]], A=A[shuffler], Y=Y[shuffler], \n",
    "                             Z_img=[image_base64(z) for z in Z[shuffler]], Z_=Z_[shuffler]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50NLmTdDKq0-"
   },
   "source": [
    "## Exercise 1: Simpson's paradox (15 min)\n",
    "\n",
    "In this exercise, we are going to dive into the notion of confounding by experimenting with a classical example: Simpson's paradox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxYHOFgjjozp"
   },
   "source": [
    "<img src=\"https://www.escardio.org/static-file/Escardio/Medias/education/covid-19/Covid-19-Webpage-banner-1170x240-opt2.jpg?mts=1594053162000.jpg\">\n",
    "\n",
    "**Context:** The world is confronted with a rare disease for which the world's public health agencies are urgently trying to find therapies. Scientists have proposed a new drug, but its effectiveness has not fully been demonstrated. As your government's top data scientist, you have been tasked with finding evidence of the drug's effectiveness. This is not a light task, your feedback will inform your country's treatment policy.\n",
    "\n",
    "There is not enough time to run a clinical study. Instead, you have access to data collected in a neighbouring country, which has already started to use the drug on patients. The dataset contains the following variables:\n",
    "\n",
    "1.   **Z**: severity of symptoms (mild=0, strong=1, critical=2)\n",
    "2.   **A**: treatment that was administered (treated=1, untreated=0)\n",
    "3.   **Y**: survival after one month (alive=1, dead=0)\n",
    "\n",
    "Good luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dISJ4VSKlN19"
   },
   "source": [
    "Let's load this dataset and print the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "K_TIIXpbK4Sr",
    "outputId": "5af66ff7-8c85-4639-e0fe-04aafe2bdf9c"
   },
   "outputs": [],
   "source": [
    "data = generate_data_ex1()\n",
    "data_table.DataTable(data, include_index=False, num_rows_per_page=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAzoS0q4dWhG"
   },
   "source": [
    "You are interested measuring the average treatment effect, that is, the chances of survival if someone *takes* vs. *does not take* the treatment. You decide to estimate the following quantity:\n",
    "$$\\mathbb{E}[Y \\mid A = 1] - \\mathbb{E}[Y \\mid A = 0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "ODoFeCzcdVFk",
    "outputId": "31bc83c7-45ba-4924-b5c9-6152482c02ab"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"A\", y=\"Y\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"Treatment (A)\")\n",
    "plt.ylabel(\"Survival (Y)\")\n",
    "plt.title(\"$\\mathbb{E}[Y \\mid A]$ for treated and untreated patients\")\n",
    "plt.show()\n",
    "\n",
    "ate = {\"conditionals\": data.loc[data.A == 1].Y.mean() - data.loc[data.A == 0].Y.mean()}\n",
    "print(\"E[Y | A = 1] - E[Y | A = 0] = \", ate[\"conditionals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DGY0A6Ni6mq"
   },
   "source": [
    "**Q:** What do you conclude?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> It looks like the treatment has a detrimental effect on survival, which suggets that it is not effective!\n",
    "\n",
    "This is really odd. You were told that this treatment is absolutely state-of-the-art and that it was likely to be effective. You start to question these conclusions and decide to dig deeper into the data.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HP8TB8fft0g"
   },
   "source": [
    "Let us now look at the profile of patients that were treated. Are they special in any way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFuM3HTxfNhY",
    "outputId": "8b2f397f-203b-44d8-94f6-af98ac4ce920"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"Z\", y=\"A\")\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"$P(A = 1 \\mid Z)$\")\n",
    "plt.xlabel(\"Symptoms ($Z$)\")\n",
    "plt.gca().set_xticklabels([\"Mild\", \"Strong\", \"Critical\"])\n",
    "plt.title(\"Probability of receiving the drug w.r.t. symptoms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IJrwz82g7g0"
   },
   "source": [
    "Interesting... 🤔\n",
    "\n",
    "**Q:** What do you observe?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> The probability of being treated clearly increases with the strength of the symptoms. This is probably due to some policy that the doctors were using to determine whether to treat patients or not.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIzsgJlWhP_L"
   },
   "source": [
    "Let's look into the relationship between the strength of the symptoms ($Z$) and the probability of survival ($Y$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "033JYYckfpj4",
    "outputId": "68c7fc94-f187-4a70-9c04-f0b388c92767"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"Z\", y=\"Y\")\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"$P(Y = 1 \\mid Z)$\")\n",
    "plt.xlabel(\"Symptoms ($Z$)\")\n",
    "plt.gca().set_xticklabels([\"Mild\", \"Strong\", \"Critical\"])\n",
    "plt.title(\"Probability of survival w.r.t. symptoms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIgsC9WniGpY"
   },
   "source": [
    "**Q:** What do you observe here?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> The probability of survival clearly decreases with the intensity of the symptoms (Z).\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvmEYKK9VORA"
   },
   "source": [
    "**A graphical perspective:** You decide to question the doctors on how they decide whether or not to treat patients. They tell you the following:\n",
    "\n",
    "> It's simple, we know that patients with stronger symptoms are less likely to survive, so we look at their symptoms and decide to treat whenever we anticipate a lower chance of survival.\n",
    "\n",
    "Let's translate this into a causal graph over the variables $A$, $Z$, and $Y$. The doctors decide to treat or not based on their assessment of the symptoms $Z$, so there needs to be an edge $Z \\rightarrow A$. The severity of the symptoms also affects the chance of survival, so there needs to be an edge $Z \\rightarrow Y$. Furthermore, we include an edge from $A \\rightarrow Y$, since we suspect that the treatment has an effect on survival.\n",
    "\n",
    "We get the following graph, where $Z$ confounds the effect of $A$ on $Y$ (that is, $Z$ is a common cause of both $A$ and $Y$):\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex1a.png\" width=\"210\">\n",
    "</p>\n",
    "\n",
    "Looking at this graph, we see that information can flow from $A$ to $Y$ through two paths:\n",
    "* $A \\leftarrow Z \\rightarrow Y$\n",
    "* $A \\rightarrow Y$\n",
    "\n",
    "This means that the difference in expectations that we measured, $\\mathbb{E}[Y \\mid A = 1] - \\mathbb{E}[Y \\mid A = 0]$, can be due to any of these paths.\n",
    "\n",
    "To get the answer we want, we need to isolate the $A \\rightarrow Y$ path. **Only this will reveal the true causal effect.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdDzBz-WsujN"
   },
   "source": [
    "**Randomization:** Now, let's assume hypothetically that you were able to collect new data and that you could randomize the treatment assignments. Instead of using $Z$ to determine $A$, you would simply flip a coin and used this to assign treatment.\n",
    "\n",
    "This would correspond to the following causal graph, where $Z$ does not affect $A$ anymore:\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex1b.png\" width=\"230\">\n",
    "</p>\n",
    "\n",
    "💡 This means that any association between $A$ and $Y$ that we measure must be due to the $A \\rightarrow Y$ path in the graph. So using the conditional expectations to measure the treatment's causal effect (as we did above) should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec5z12SGntxb"
   },
   "source": [
    "\n",
    "🟢 Let's regenerate the dataset, this time using randomization to assign the treatments. Below, you will find the function that was used to generate the data. Modify it so that treatments are randomly with probability 50%. Also, take time to understand the rest of the code.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Spoiler: click here to reveal solution</summary>\n",
    "\n",
    "You need to change this line:\n",
    "<br />\n",
    "<code>policy = [0.05, 0.5, 0.9]</code>\n",
    "<br />\n",
    "to\n",
    "<br />\n",
    "<code>policy = [0.5, 0.5, 0.5]</code>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjc-9OOhFInM",
    "outputId": "bfcea3b0-bdd0-4b36-b097-3f71802e6fd4"
   },
   "outputs": [],
   "source": [
    "def generate_data_ex1_randomized(n_samples=10000, seed=42):\n",
    "    random = np.random.RandomState(seed)\n",
    "\n",
    "    Z = random.randint(0, 3, size=n_samples)\n",
    "    \n",
    "    policy = [0.05, 0.5, 0.9]  # Probability of treatment for each Z\n",
    "    A = np.array([random.rand() <= policy[z] for z in Z]).astype(int)\n",
    "\n",
    "    # Value of Z1: 0     1    2\n",
    "    survival = [[0.75, 0.25, 0.1],  # Untreated (A = 0)\n",
    "                [0.95, 0.75, 0.25]] # Treated (A = 1)\n",
    "    Y = np.fromiter((random.rand() <= survival[A[i]][Z[i]] for i in range(n_samples)), dtype=int)\n",
    "\n",
    "    return pd.DataFrame(dict(Z=Z, A=A, Y=Y))\n",
    "\n",
    "data = generate_data_ex1_randomized()\n",
    "data_table.DataTable(data, include_index=False, num_rows_per_page=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBtDfeKMFTjK",
    "outputId": "43208f67-77e3-4ee1-d72b-55a72719e28e"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"Z\", y=\"A\")\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"$P(A = 1 \\mid Z)$\")\n",
    "plt.xlabel(\"Symptoms ($Z$)\")\n",
    "plt.gca().set_xticklabels([\"Mild\", \"Strong\", \"Critical\"])\n",
    "plt.title(\"Probability of receiving the drug w.r.t. symptoms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3uxVcA0FehJ"
   },
   "source": [
    "As expected, the strength of the symptoms does not affect the treatment assignment anymore.\n",
    "\n",
    "Let's now look at $\\mathbb{E}[Y \\mid A = 1] - \\mathbb{E}[Y \\mid A = 0]$ again. This will correspond to the actual *average treatment effect* of our drug: $$\\mathbb{E}[Y \\mid do(A = 1)] - \\mathbb{E}[Y \\mid do(A = 0)],$$\n",
    "where the $do()$-operator denotes actively setting $A=a$ for $a\\in\\{0,1\\}$ (this is also called *intervention*).\n",
    "\n",
    "In short, the interventional (or causal) effect is the one we sought and doing the randomization before checking for conditionals turns out to be equivalent to the causal effect we seek (at least in this example that we consider here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWr0qK8JFQdB",
    "outputId": "065c0870-aff4-48c8-8232-64f761a9365b"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"A\", y=\"Y\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"Treatment (A)\")\n",
    "plt.ylabel(\"Survival (Y)\")\n",
    "plt.title(\"$\\mathbb{E}[Y \\mid A]$ for treated and untreated patients\")\n",
    "plt.show()\n",
    "\n",
    "ate[\"randomization\"] = data.loc[data.A == 1].Y.mean() - data.loc[data.A == 0].Y.mean()\n",
    "print(\"E[Y | A = 1] - E[Y | A = 0] = \", ate[\"randomization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysfxAa8RFjD8"
   },
   "source": [
    "What?! The drug has a positive effect now onto survival? So, people have a better chance of surviving if they take the drug? Let's compare to what we had previously measured:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwBIO5HVr19f",
    "outputId": "d43814a8-2cfd-47b7-a74e-75ccc5fbbb1a"
   },
   "outputs": [],
   "source": [
    "print(\"Average treatment effect of drug by estimation technique:\")\n",
    "pprint_ates(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-ScjeDNugk3"
   },
   "source": [
    "Indeed, the drug has a positive effect on survival. Our assessment based on conditional probabilities was **biased due to confounding** (due to the common cause, $Z$, that is, doctors were deciding who to treat based on severity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWGpEUiCsQ-C"
   },
   "source": [
    "**Final words:** Now you know the difference between conditioning and acting. When conducting data analysis, it is important to think about the nature of the data, i.e., how it could have been generated. By failing to do so, you would have concluded that the treatment was ineffective, while you have an effective solution to the rare disease right in front of you.\n",
    "\n",
    "**Causal inference from observational data:** This is great, but we reached this conclusion by altering the data collection process via randomization, which we often cannot do. In what follows, we will explore methods that enable us to accurately estimate the effect of interventions from biased data that we observe (observational data), without being able to interfere with the collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UefDxwkUKzbo"
   },
   "source": [
    "## Exercise 2: Parent adjustment (20 min)\n",
    "\n",
    "In this exercise, we will derive and implement a very simple estimator of the average treatment effect. This will help us properly assess the causal effect of the drug in Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqCNUwCU6jrV"
   },
   "source": [
    "Recall the causal graph that describes our data:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex1a.png\" width=\"210\">\n",
    "</p>\n",
    "\n",
    "Assuming all relevant variables are in this graph, we can factorize the joint distribution as follows:\n",
    "\\begin{align}\n",
    "P(a, y, z) &= P(a | z) \\cdot P(y | a, z) \\cdot P(z)\n",
    "\\end{align}\n",
    "\n",
    "Now, we want to reason about interventions, i.e., what would be the distribution of $Y$ in a world where:\n",
    "1. We forced everyone to take the treatment ($A = 1$)\n",
    "2. We forced everyone not to take it ($A = 0$). \n",
    "\n",
    "In both cases, we are imagining a situation where we fully control the value of $A$ and it is affected by no external forces. This corresponds to the following graph, where the hammer is used to show where we intervene:\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex2.png\" width=\"230\">\n",
    "</p>\n",
    "\n",
    "**Identification:** How do we estimate quantities that live in such interventional worlds, e.g., $\\mathbb{E}[Y | do(A = a)]$, if we only have access to our confounded observational data? The key is to use our knowledge of the causal graph to perform a set of operations that convert the causal quantity (with $do$ operators) into an expression that we can evaluate from our data (with no $do$ operators). Performing such a conversion is called \"identifying a causal estimand\". Let's see an example.\n",
    "\n",
    "<br />\n",
    "\n",
    "**Parent Adjustment**\n",
    "\n",
    "The first identification technique that we will see is called *parent adjustment*.\n",
    "To begin, notice that the only difference between the full causal graph (for the observational data) and the one corresponding to an intervention is that the edge $Z \\rightarrow A$ is removed. Everything else stays the same.\n",
    "\n",
    "This means that we can write down the joint distribution of $A$, $Y$, $Z$ under an intervention $do(A = a')$ as follows:\n",
    "\\begin{align}\n",
    "P(a, y, z \\mid do(A=a')) &= P(y \\mid a', z) \\cdot P(z) \\cdot \\delta_{a=a'},\n",
    "\\end{align}\n",
    "where most terms are simply copied from the expression of the observational joint distribution and where $\\delta_{a = a'} = 1$ if and only if $A = a'$ and $0$ otherwise. This last term simply means that the interventional distribution has zero density for all values of $a$ that don't correspond to the value of the intervention ($a'$), which makes sense since the intervention forces $A = a'$ (no other values could be observed).\n",
    "\n",
    "\n",
    "Now, we are only interested in the distribution of $Y$ in this interventional world, so we marginalize out the other variables:\n",
    "\\begin{align}\n",
    "P(y \\mid do(A=a')) &= \\sum_a \\sum_z P(a, y, z \\mid do(A=a'))\\\\\n",
    "                & <\\text{Replacing the above by its definition}>\\\\\n",
    "                &= \\sum_a \\sum_z P(y \\mid a', z) \\cdot P(z) \\cdot \\delta_{a=a'}\\\\\n",
    "                & <\\text{Density is zero for all $a \\not= a'$}>\\\\\n",
    "                &= 0 + \\sum_z P(y \\mid a', z) \\cdot P(z) \\cdot 1\\\\\n",
    "                & <\\text{Cleaning up a bit}>\\\\\n",
    "                &= \\sum_z P(y \\mid a', z) \\cdot P(z)\\\\\n",
    "\\end{align}\n",
    "\n",
    "And we are done! We have successfully *identified* the causal estimand, since we went from a quantity that contained $do(.)$ operators to one that does not. 🎉\n",
    "\n",
    "This result simply tells us that, to estimate the interventional distribution correctly, we need to evaluate the conditional distribution of $Y$ given $A$ and $Z$ in every subgroup (stratum) of our population corresponding to a specific value of $Z$. If you read the econometrics or social sciences literature, you may see this expression termed *standardization*, they mean the same thing.\n",
    "\n",
    "**Average treatment effect:** Let us now use parent adjustment to estimate the quantity that we care about, the average treatment effect of our drug:\n",
    "$$ATE = \\mathbb{E}[Y \\mid do(A = 1)] - \\mathbb{E}[Y \\mid do(A = 0)],$$\n",
    "where\n",
    "\\begin{align}\n",
    "\\mathbb{E}[Y \\mid do(A = a)] &= \\sum_y y \\cdot P(y \\mid do(A = a))\\\\\n",
    "                          & <\\text{Explicitly summing over values of $y$}>\\\\\n",
    "                          &= 1 \\cdot P(Y=1 \\mid do(A = a)) + 0 \\cdot P(Y=0 | do(A = a))\\\\\n",
    "                          & <\\text{By parent adjustment}>\\\\\n",
    "                          &= \\sum_z P(Y = 1 \\mid a, z) \\cdot P(z).\n",
    "\\end{align}\n",
    "\n",
    "If we implement this correctly, we should find the same value as what we obtained via randomization.\n",
    "\n",
    "<br />\n",
    "\n",
    "🟢 Take a look at the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7hy6HjrK3sM",
    "outputId": "663844e2-8052-47b0-822d-5a2b67dd29b5"
   },
   "outputs": [],
   "source": [
    "def parent_adjustment_estimator(data, a):\n",
    "    \"\"\"\n",
    "    Use parent adjustment to estimate E[Y | do(A = a)]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pd.DataFrame\n",
    "        The observational data\n",
    "    a: int\n",
    "        The value of the intervention\n",
    "\n",
    "    \"\"\"\n",
    "    estimate = 0\n",
    "\n",
    "    # For every possible value of Z\n",
    "    for z, subpop_data in data.groupby(\"Z\"):\n",
    "\n",
    "        # Estimate P(z)\n",
    "        p_z = subpop_data.shape[0] / data.shape[0]\n",
    "\n",
    "        # Estimate E[Y | A=a, Z=z]\n",
    "        # Here, we are using discrete data, so we simply need to look at the mean\n",
    "        # value of Y in some slices of our data. If you were using continuous data\n",
    "        # you could use a supervised learning model of your choice to fit this\n",
    "        # function (e.g., a neural network).\n",
    "        expected_y = subpop_data.loc[subpop_data.A == a].Y.mean()\n",
    "        \n",
    "        # Add the value of this strata to the estimator\n",
    "        estimate += expected_y * p_z\n",
    "\n",
    "    return estimate\n",
    "\n",
    "\n",
    "# Regenerate a fresh copy of the observational data\n",
    "# We are not cheating by randomizing here. \n",
    "# There is confounding bias.\n",
    "data = generate_data_ex1()\n",
    "\n",
    "# Estimate the treatment effect using parent adjustment\n",
    "ate[\"parent_adj\"] = parent_adjustment_estimator(data, a=1) - parent_adjustment_estimator(data, a=0)\n",
    "\n",
    "pprint_ates(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN7brB-jGNru"
   },
   "source": [
    "Wow, it works! Parent adjustment enabled us to use biased observational data to reach the same conclusion as we previously reached with randomization. We didn't even have to interfere with data collection. Note that the values do not match exactly since we use finite data, but in the limit the values become identical.\n",
    "\n",
    "\n",
    "**A more fancy estimator**\n",
    "\n",
    "In our example, all the variables are discrete, so we were able to estimate $\\mathbb{E}[Y | A=a, Z=z]$ by filtering the data and counting the occurrence of some values. However, we could have prefered to rely on a model, trained to estimate this quantity by supervised learning. In what follows, we will see how such a model can be integrated in the estimator.\n",
    "\n",
    "Furthermore, we could have approximated $\\mathbb{E}[Y \\mid do(A = a)]$ using a Monte-Carlo approximation. A what approximation? When we have a sample of size $n$, $\\mathcal{S} = \\{ x_1, \\dots, x_n \\}$, from some distribution $P(X)$, we can estimate the expectation of some quantity $f(X)$ w.r.t. $X$ as $\\mathbb{E}_X \\left[f(X)\\right] \\approx \\frac{1}{n} \\sum_{x_i} f(x_i)$. This means that we can rely on the following approximation:\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[Y \\mid do(A = a)] &= \\sum_z P(Y = 1 \\mid a', z) \\cdot P(z)\\\\\n",
    "                                 & <\\text{Since Y is a binary variable}>\\\\\n",
    "                                 &= \\sum_z \\mathbb{E}[Y \\mid a', z] \\cdot P(z)\\\\\n",
    "                                 & <\\text{Noting that this is an expectation w.r.t. $Z$}>\\\\\n",
    "                                   &= \\mathbb{E}_Z \\left[ \\mathbb{E}[Y \\mid a', z] \\right]\\\\\n",
    "                                & <\\text{Monte-Carlo approximation using observed values of $Z$ in our data}>\\\\\n",
    "                                   &\\approx \\dfrac{1}{n} \\sum_{z_i} E[Y \\mid a', z_i]\n",
    "\\end{align}\n",
    "\n",
    "Ok, let's implement this estimator.\n",
    "\n",
    "🟢 Look at the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCHGnicfW6ho",
    "outputId": "b73e8d2b-c7e9-4e5f-8e3f-ccceca747e1f"
   },
   "outputs": [],
   "source": [
    "class OutcomeModel(object):\n",
    "    \"\"\"\n",
    "    A model that estimates E[Y | A, Z].\n",
    "\n",
    "    This will be very simple in our case but, in practice, this could\n",
    "    have been any fancy model, such as a neural network trained by\n",
    "    supervised learning.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.conditionals = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the model\n",
    "\n",
    "        \"\"\"\n",
    "        # We're just going to count the frequency of each Z for each A and store it.\n",
    "        # This is a dummy model, just for the purpose of our example.\n",
    "        self.conditionals = np.zeros((len(data.A.unique()), len(data.Z.unique())))\n",
    "        for (a, z), subset in data.groupby([\"A\", \"Z\"]):\n",
    "            self.conditionals[a, z] = subset.Y.mean()\n",
    "\n",
    "    def predict(self, a, z):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "\n",
    "        \"\"\"\n",
    "        return self.conditionals[a, z]\n",
    "\n",
    "\n",
    "def parent_adjustment_montecarlo_estimator(data, a, seed=42):\n",
    "    \"\"\"\n",
    "    Use parent adjustment to estimate E[y | do(A = a)].\n",
    "    Here, we use the empirical data distribution to estimate the expectation over Z.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pd.DataFrame\n",
    "        The observational data\n",
    "    a: int\n",
    "        The value of the intervention\n",
    "\n",
    "    \"\"\"\n",
    "    # Fit an outcome model to predict E[Y | do(A = a)]\n",
    "    outcome_model = OutcomeModel()\n",
    "    outcome_model.fit(data)\n",
    "\n",
    "    # Average the outcome using the empirical distribution of Z\n",
    "    return np.mean([outcome_model.predict(a, z) for z in data.Z])\n",
    "\n",
    "\n",
    "ate[\"parent_adj_mc\"] = parent_adjustment_montecarlo_estimator(data, a=1) - parent_adjustment_montecarlo_estimator(data, a=0)\n",
    "pprint_ates(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbrwGsnSY3F0"
   },
   "source": [
    "**Remark:** This last estimator is quite interesting, since it makes use of a model, which could be of arbitrary nature. For example, the confounder is of a complex nature, such as an image (e.g., an x-ray) or text (e.g., an electronic health record), we could define the outcome model as being a neural network and simply train it via supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfulnY0zEGoj"
   },
   "source": [
    "## Exercise 3: From parent to back-door adjustment (20 min)\n",
    "\n",
    "Do we really need to adjust using all causal parents? What if I can't measure all the parent variables (e.g., budget constraints, missing data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfbgWNZi7vds"
   },
   "source": [
    "When we postulate a causal graph and we assume that the data distribution is generated according to it, we make an assumption about the conditional independences that exist in the distribution: *the Markov property*.\n",
    "\n",
    "**Markovian assumption:** We assume that two (sets of) variables $A$, $B$ are independent conditioned on a (set of) variables $C$, if $C$ blocks all paths between $A$ and $B$ in the graph. This is typically written as follows:\n",
    "$$\n",
    "A \\perp \\!\\! \\perp_G B \\mid C \\Rightarrow A \\perp \\!\\! \\perp B \\mid C,\n",
    "$$\n",
    "where $\\perp \\!\\! \\perp_G$ denotes separation in the graph (i.e., d-separation), while $\\perp \\!\\! \\perp$ denotes independence in distribution. \n",
    "\n",
    "<br />\n",
    "\n",
    "Let's experiment with this a little bit. We will start by generating data according to the following graph:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex3a.png\" width=\"350\">\n",
    "</p>\n",
    "\n",
    "and look at some dependencies between the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qTFaUJu_46g",
    "outputId": "9cb0d45f-6333-4296-acca-ffb199c29024"
   },
   "outputs": [],
   "source": [
    "def generate_data_ex3a(n_samples=10000, seed=42):\n",
    "    random = np.random.RandomState(seed)\n",
    "    C = random.randint(0, 2, n_samples).astype(int)\n",
    "    A = np.array([random.rand() < (0.1 if c == 0 else 0.8) for c in C]).astype(int)\n",
    "    B = np.array([random.rand() < (0.3 if c == 0 else 0.95) for c in C]).astype(int)\n",
    "    return pd.DataFrame(dict(A=A, B=B, C=C))\n",
    "\n",
    "# Generate the data\n",
    "data = generate_data_ex3a()\n",
    "\n",
    "# Plot dependencies\n",
    "f, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "sns.barplot(data=data, x=\"A\", y=\"B\", ax=ax1)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_title(\"$P(B = 1 \\mid A)$\")\n",
    "\n",
    "sns.barplot(data=data.loc[data.C == 0], x=\"A\", y=\"B\", ax=ax2)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_title(\"$P(B = 1 \\mid A, C=0)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjSfNZZpAuob"
   },
   "source": [
    "**Q:** What do you observe here? Does it correspond to your graphical intuition?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> We see that $B$ depends on $A$ if we don't condition on $C$ (we observed that the distribution of $B$ changes strongly for different values of $A$), but that it doesn't when we condition on $C$ (we observed that the distribution of $B$ did not change with $A$, as before, as soon as we knew about the value of $C$). This does correspond to what we can read off from the graph, since $C$ blocks all paths from $A$ to $B$ (regardless of edge orientation) through which statistical information can flow.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr6qk7fkBfSH"
   },
   "source": [
    "Now, let's do the same thing for the following graph:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex3b.png\" width=\"350\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10w4Gkm4Bl_T",
    "outputId": "5f70ccaf-6715-4b12-df3c-b2aba160d37a"
   },
   "outputs": [],
   "source": [
    "def generate_data_ex3b(n_samples=10000, seed=42):\n",
    "    random = np.random.RandomState(seed)\n",
    "    A = random.randint(0, 2, n_samples).astype(int)\n",
    "    C = np.array([random.rand() < (0.1 if a == 0 else 0.8) for a in A]).astype(int)\n",
    "    B = np.array([random.rand() < (0.3 if c == 0 else 0.95) for c in C]).astype(int)\n",
    "    return pd.DataFrame(dict(A=A, B=B, C=C))\n",
    "\n",
    "# Generate the data\n",
    "data = generate_data_ex3b()\n",
    "\n",
    "# Plot dependencies\n",
    "f, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "sns.barplot(data=data, x=\"A\", y=\"B\", ax=ax1)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_title(\"$P(B = 1 \\mid A)$\")\n",
    "\n",
    "sns.barplot(data=data.loc[data.C == 0], x=\"A\", y=\"B\", ax=ax2)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_title(\"$P(B = 1 \\mid A, C=0)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wy8i8rzB1BN"
   },
   "source": [
    "**Q:** What do you observe here? Does it correspond to your graphical intuition?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> Again, we see the same thing: $B$ depends on $A$ if we don't condition on $C$, but it doesn't when we condition on $C$. This does correspond to what we can read off from the graph, since $C$ blocks all paths from $A$ to $B$ through which statistical information can flow.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJZI5QLqB9G4"
   },
   "source": [
    "Finally, let's repeat the same thing for the following graph:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex3c.png\" width=\"350\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKxphwRuCIpg",
    "outputId": "b6827c76-62ac-4833-db94-3c09f6a64231"
   },
   "outputs": [],
   "source": [
    "def generate_data_ex3b(n_samples=10000, seed=43):\n",
    "    random = np.random.RandomState(seed)\n",
    "    A = random.randint(0, 2, n_samples).astype(int)\n",
    "    B = random.randint(0, 2, n_samples).astype(int)\n",
    "    probas = random.rand(2, 2)\n",
    "    C = np.array([random.rand() < probas[a, b] for a, b in zip(A, B)]).astype(int)\n",
    "    return pd.DataFrame(dict(A=A, B=B, C=C))\n",
    "\n",
    "# Generate the data\n",
    "data = generate_data_ex3b()\n",
    "\n",
    "# Plot dependencies\n",
    "f, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "sns.barplot(data=data, x=\"A\", y=\"B\", ax=ax1)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_title(\"$P(B = 1 \\mid A)$\")\n",
    "\n",
    "sns.barplot(data=data.loc[data.C == 0], x=\"A\", y=\"B\", ax=ax2)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_title(\"$P(B = 1 \\mid A, C=0)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJcmlDUBCnT4"
   },
   "source": [
    "**Q:** What do you observe here? Does it correspond to your graphical intuition?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> This one is very strange. 🙀 We see that $A$ and $B$ are independent, unless we condition on $C$, which makes them dependent. The $A \\rightarrow C \\leftarrow B$ path is called a v-structure and it has the special property of being blocked, unless we condition on $C$ which opens it. Of note, this special independence pattern only holds when $A$ and $B$ are not directly connected by a $A \\rightarrow B$ or $A \\leftarrow B$ edge.\n",
    "\n",
    "\n",
    "Let's see an example. Assume that we have the following:\n",
    "* A: being an athlete or not\n",
    "* B: difficulty level of the test\n",
    "* C: score in physical test\n",
    "\n",
    "Knowing that someone is an athlete ($A$) reveals nothing about the difficulty of some arbitrary physical test ($B$); these quantities are independent. However, knowing that someone is an athelete ($A$) and achieved a very low score in the physical test ($C$) reveals that the test was probably very hard ($B$). Therefore, knowing about $C$ in addition to $A$ changed our belief in $B$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKwEbxPbdCh9"
   },
   "source": [
    "**Putting this into practice:** Ok! Now let's use this new property that we just learned about. Suppose that we have data generated according to the following causal graph:\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex3d.png\" width=\"230\">\n",
    "</p>\n",
    "\n",
    "Let's add the following complication: we measure all variables, except $Z_1$ which requires an expensive measurement device that we can't afford.\n",
    "\n",
    "Huh? We don't measure all parents of $A$ anymore, so we can't use parent adjustment. What should we do? Should we ask for money to buy the expensive device? 💰💰💰\n",
    "\n",
    "Let's explore the data a little bit and then decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwfqxYegLQ9w"
   },
   "source": [
    "🟢 Take a look at the data-generating function and try to get an intuition of why it respects the structure of the causal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MvLI6fdLXbU"
   },
   "outputs": [],
   "source": [
    "def generate_data_ex3d(n_samples=100000, randomize=False, seed=0):\n",
    "    \"\"\"\n",
    "    Note: we use a really big sample size to avoid any small sample glitches\n",
    "\n",
    "    \"\"\"\n",
    "    random = np.random.RandomState(seed)\n",
    "\n",
    "    n_z = 3  # Number of possible values for Z1 and Z2\n",
    "\n",
    "    # We first sample the value of Z1 randomly (it has no causal parents)\n",
    "    Z1 = random.randint(0, n_z, size=n_samples)\n",
    "\n",
    "    # Since Z1 -> Z2, we will sample the value of Z2, conditioned on Z1\n",
    "    # Let's generate random conditional probabilities\n",
    "    p_z2_given_z1 = random.rand(n_z, n_z)\n",
    "    p_z2_given_z1 /= p_z2_given_z1.sum(axis=1)[:, None]\n",
    "    Z2 = np.array([random.choice(np.arange(n_z), p=p_z2_given_z1[z1]) for z1 in Z1]).astype(int)\n",
    "    \n",
    "    # Probability of treatment for each Z\n",
    "    if randomize:\n",
    "        # If we are randomizing treatment, we set the proba to 0.5 for any (z1, z2)\n",
    "        policy = np.ones((n_z, n_z)) * 0.5\n",
    "    else:\n",
    "        # Here, we are not randomizing so the value of A depends on (z1, z2).\n",
    "        # Let's generate random conditional probabilities P(A | Z1, Z2)\n",
    "        policy = random.rand(n_z, n_z)\n",
    "    \n",
    "    # Sample treatment assignment based on the values of Z1 and Z2\n",
    "    A = np.array([random.rand() <= policy[z1, z2] for z1, z2 in zip(Z1, Z2)]).astype(int)\n",
    "\n",
    "    # In the graph, we have Z2 -> Y, but not Z1 -> Y.\n",
    "    # Hence, the value of Y only depends on Z2 and A\n",
    "    #      Value of Z2: 0     1    2\n",
    "    p_y_given_a_z2 = [[0.75, 0.25, 0.1],   # Untreated (A = 0)\n",
    "                      [0.95, 0.75, 0.25]]  # Treated (A = 1)\n",
    "    Y = np.fromiter((random.rand() <= p_y_given_a_z2[A[i]][Z2[i]] for i in range(n_samples)), dtype=int)\n",
    "\n",
    "    return pd.DataFrame(dict(Z1=Z1, Z2=Z2, A=A, Y=Y))\n",
    "\n",
    "\n",
    "data = generate_data_ex3d(randomize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDTf8pECPgzw"
   },
   "source": [
    "Let's first see if $A$ and $Z_1, Z_2$ are dependent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uat6bjOIuAD6",
    "outputId": "7cf53ffa-ca55-4a40-888a-b4f9318847ef"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "sns.barplot(data=data, x=\"Z1\", y=\"A\", ax=ax1)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel(\"$Z_1$\")\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_title(\"$P(A = 1 \\mid Z_1)$\")\n",
    "\n",
    "sns.barplot(data=data, x=\"Z2\", y=\"A\", ax=ax2)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_xlabel(\"$Z_2$\")\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_title(\"$P(A = 1 \\mid Z_2)$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96lNk7TnSs2Q"
   },
   "source": [
    "**Q:** What do you observe here? Does it correspond to your graphical intuition?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> Clearly, the distribution of $A$ changes with $Z_1$ and $Z_2$, so there is a dependency between $A$ and each of these variables. This does correspond to our graphical intuition, since they are directly connected via $Z_1 \\rightarrow A$ and $Z_2 \\rightarrow A$ edges.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6qlxIliJQrq"
   },
   "source": [
    "Similarly, let's check if $Z_1$, $Z_2$ are associated with $Y$ when we condition on $A$ to block any flow of information via the $Z_i \\rightarrow A \\rightarrow Y$ path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhAgvAChPxS5",
    "outputId": "104d2c97-2a9a-445f-b42b-78eb33c0d3e1"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "sns.barplot(data=data.loc[data.A == 0], x=\"Z1\", y=\"Y\", ax=ax1)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel(\"$Z_1$\")\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_title(\"$P(Y = 1 \\mid A = 0, Z_1)$\")\n",
    "\n",
    "sns.barplot(data=data.loc[data.A == 0], x=\"Z2\", y=\"Y\", ax=ax2)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_xlabel(\"$Z_2$\")\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_title(\"$P(Y = 1 \\mid A = 0, Z_2)$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACsm3fVkSZDY"
   },
   "source": [
    "**Q:** What do you observe here? Does it correspond to your graphical intuition?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> Clearly, the distribution of $Y$ changes with $Z_1$ and $Z_2$, so there is a dependency between $Y$ and each of these variables, even when we block the $A \\rightarrow Y$ path. This does correspond to our graphical intuition, since $Z_2$ and $Y$ are directly connected via the $Z_2 \\rightarrow Y$ edge and there is a path from $Z_1$ to $Y$: $Z_1 \\rightarrow Z_2 \\rightarrow Y$.\n",
    "\n",
    "This also tells us that $Z_1$ and $Z_2$ are confounders, since they are both i) associated with $A$ and ii) are still associated with $Y$ when we block the $A \\rightarrow Y$ path.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omo860skJ8xm"
   },
   "source": [
    "Ok. Now looking at the graph, we see that our treatment variable $A$ is connected to $Y$ via the following paths:\n",
    "* $A \\rightarrow Y$\n",
    "* $A \\leftarrow Z_1 \\rightarrow Z_2 \\rightarrow Y$\n",
    "* $A \\leftarrow Z_2 \\rightarrow Y$\n",
    "\n",
    "The first one is the causal path, who's strength we want to measure. The last two paths are called *back-door paths*, since they point into $A$, i.e., they enter it through the back door.\n",
    "\n",
    "💡 Notice how conditioning on $Z_2$ should block all of the back-door paths! (even the path with the unobserved $Z_1$)\n",
    "\n",
    "Let's check if this is supported by the data. If it is, we should see that $Z_1$ is independent of $Y$ when we condition on both i) $A$, to block the $Z_1 \\rightarrow A \\rightarrow Y$ path and ii) $Z_2$ to block all other paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXvnXzNFUTW_",
    "outputId": "143a8f17-fdfe-477e-897a-0bc17c8b025a"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=data.loc[(data.A == 0) & (data.Z2 == 0)], x=\"Z1\", y=\"Y\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"$Z_1$\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"$P(Y = 1 \\mid A = 0, Z_2 = 0, Z_1)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDCMLX74VHi0"
   },
   "source": [
    "**Q:** What do you conclude?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "<strong>A:</strong> Changing the value of $Z_1$ has no effect here. In other words, we observe that $P(Y \\mid A, Z_1, Z_2) = P(Y \\mid A, Z_2)$, i.e., $Z_1 \\perp \\!\\! \\perp Y \\mid \\{A, Z_2\\}$. This was expected, since all paths through which statistical information can flow are blocked (see the graph).\n",
    "\n",
    "In conclusion, this means that there is absolutely no point in conditioning on $Z_1$ if we know the value of $Z_2$ (and although $Z_1$ is not measured, there is nothing to worry about, because of $Z_2$ we are good to go and we don't need to spend the money for measuring $Z_1$).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la1IWVEQtdwH"
   },
   "source": [
    "### On to estimating the treatment effect\n",
    "\n",
    "Now, let's use this new knowledge of our data to modify our parent adjustment formula:\n",
    "\n",
    "\\begin{align}\n",
    "P(y \\mid do(A=a'))\n",
    "                &= \\sum_{a, z_1, z_2} P(a, y, z_1, z_2 \\mid do(A=a'))\\\\\n",
    "                & <\\text{Simply reusing our derivation for parent adjustment (see ex. 2)}>\\\\\n",
    "                &= \\sum_{z_1, z_2} P(y \\mid a', z_1, z_2) \\cdot P(z_1, z_2)\\\\\n",
    "                & <\\text{Factorizing the joint distribution of $Z_1$ and $Z_2$}>\\\\\n",
    "                &= \\sum_{z_1}\\sum_{z_2} P(y \\mid a', z_1, z_2) \\cdot P(z_1 | z_2) \\cdot P(z_2)\\\\\n",
    "                & <\\text{Using our knowledge that $P(Y \\mid A, Z_1, Z_2) = P(Y \\mid A, Z_2)$}>\\\\\n",
    "                &= \\sum_{z_2} \\left[\\sum_{z_1} P(y \\mid a', z_2) \\cdot P(z_1 | z_2)\\right] \\cdot P(z_2)\\\\\n",
    "                & <\\text{Noting that $P(y \\mid a', z_2)$ is constant in the sum over $z_1$}>\\\\\n",
    "                &= \\sum_{z_2} P(y \\mid a', z_2) \\left[\\sum_{z_1} \\cdot P(z_1 | z_2)\\right] \\cdot P(z_2)\\\\\n",
    "                & <\\text{Since $\\sum_{z_1} P(z_1 | z_2) = 1$}>\\\\\n",
    "                &= \\sum_{z_2} P(y \\mid a', z_2) \\cdot P(z_2)\\\\\n",
    "\\end{align}\n",
    "\n",
    "And so we do not even need to adjust for $Z_1$ here! 🎉\n",
    "\n",
    "\n",
    "**What's the general pattern here?** Pearl (2009) defines the *Back-door criterion*, which states that whenever we can find a subset $\\mathcal{Z}^\\star$ of the parents of $A$ that blocks all backdoor paths from $Y$ into $A$, called a valid *back-door adjustment set*, we can use it to adjust in replacement of the full set of parents.\n",
    "\n",
    "This is very convenient when some parent variables are not measured!\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "🟢 Take a look at the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BW_aWohzgvEa"
   },
   "outputs": [],
   "source": [
    "def backdoor_adjustment_estimator(data, a, adjustment_set=[]):\n",
    "    \"\"\"\n",
    "    Use backdoor adjustment to estimate E[Y | do(A = a)]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pd.DataFrame\n",
    "        The observational data\n",
    "    a: int\n",
    "        The value of the intervention\n",
    "\n",
    "    \"\"\"\n",
    "    estimate = 0\n",
    "\n",
    "    # For every possible value zstar of all variables in our adjustment set Zstar\n",
    "    for z, subpop_data in data.groupby(adjustment_set):\n",
    "\n",
    "        # Estimate P(zstar)\n",
    "        p_z = subpop_data.shape[0] / data.shape[0]\n",
    "\n",
    "        # Estimate E[Y | A=a, Zstar=zstar]\n",
    "        # Here, we are using discrete data, so we simply need to look at the mean\n",
    "        # value of Y in some slices of our data. If you were using continuous data\n",
    "        # you could use a supervised learning model of your choice to fit this\n",
    "        # function (e.g., a neural network).\n",
    "        expected_y = subpop_data.loc[subpop_data.A == a].Y.mean()\n",
    "        \n",
    "        # Add the value of this strata to the estimator\n",
    "        estimate += expected_y * p_z\n",
    "\n",
    "    return estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU5ueu5aUbYF"
   },
   "source": [
    "Now, let's compare the following estimates of the average treatment effect:\n",
    "* randomization (ground truth)\n",
    "* using conditional probabilities only (shouldn't work since $A \\rightarrow Z_2 \\rightarrow Y$ remains unblocked)\n",
    "* parent adjustment (if we had the full set of parents)\n",
    "* adjusting for $Z_1$ only (shouldn't work)\n",
    "* backdoor adjustment (via $Z_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62J0NoEWPa58",
    "outputId": "66a2e8cf-74bb-49dc-ed4e-e1196cc29832"
   },
   "outputs": [],
   "source": [
    "def get_randomized_ate():\n",
    "    data_rando = generate_data_ex3d(randomize=True)\n",
    "    return data_rando.loc[data_rando.A == 1].Y.mean() - data_rando.loc[data_rando.A == 0].Y.mean()\n",
    "\n",
    "ate = {}\n",
    "ate[\"Conditionals (dummy)\"] = data.loc[data.A == 1].Y.mean() - data.loc[data.A == 0].Y.mean()\n",
    "ate[\"Backdoor via Z1 (dummy)\"] = backdoor_adjustment_estimator(data, a=1, adjustment_set=[\"Z1\"]) - backdoor_adjustment_estimator(data, a=0, adjustment_set=[\"Z1\"])\n",
    "ate[\"Randomization\"] = get_randomized_ate()\n",
    "ate[\"Parent adjustment\"] = backdoor_adjustment_estimator(data, a=1, adjustment_set=[\"Z1\", \"Z2\"]) - backdoor_adjustment_estimator(data, a=0, adjustment_set=[\"Z1\", \"Z2\"])\n",
    "ate[\"Backdoor via Z2\"] = backdoor_adjustment_estimator(data, a=1, adjustment_set=[\"Z2\"]) - backdoor_adjustment_estimator(data, a=0, adjustment_set=[\"Z2\"])\n",
    "\n",
    "print(\"Average treatment effect by estimation method:\")\n",
    "pprint_ates(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRgkw4kbV8kO"
   },
   "source": [
    "As you can see, we successfully estimate the average treatment effect by adjusting for $Z_2$ only. So we won't need to ask for money to buy the fancy measurement device for $Z_1$. Great! 💰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRN41p6fSKzB"
   },
   "source": [
    "## Exercise 4: Estimation via machine learning (15 min)\n",
    "\n",
    "Everything we did today can work with complex high-dimensional data, such as images or text. In this exercise, we will see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpZL7ma3wKLA"
   },
   "source": [
    "By now, you're a real pro at causal inference. You decided to post an online add to advertise your treatment effect estimation skills. 😎\n",
    "\n",
    "The first contract comes in. Your customer, a veterinarian, needs help with assessing the effectiveness of a treatment. By discussing with the customer, you understand that this is a simple case of ATE estimation with confounding. The nature of the confounding comes from the fact that the data was collected by a human, which was likely to assign treatment based on anticipation of the outcome, as in **Exercise 1**. The graph is as follows:\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ex1a.png\" width=\"210\">\n",
    "</p>\n",
    "\n",
    "Seems easy enough.\n",
    "\n",
    "<br />\n",
    "\n",
    "Then, the customer tells you that they lost the detailed information about the animals that were treated. All that they have is a picture that was taken when they checked into the clinic. \n",
    "\n",
    "How does the customer expect us to use such data?! We certaintly cannot use parent adjustment and adjust for the value of every single pixel. Should you turn down this contract? What can you do? How do you deal with such data? 🙀\n",
    "\n",
    "<br />\n",
    "\n",
    "It turns out that we can use models, e.g., neural networks, as components of the estimators that we have seen in this tutorial. Now, we will see an example of an inverse probability weighting estimator, where the propensity score $P(a | z)$ is estimated using a convolutional neural network.\n",
    "\n",
    "<br />\n",
    "\n",
    "🟢 Start by loading the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9132aab26a4d4f14bbb82a6a1f322291",
      "117353bf99e44b89b1c2556a7018ef8a",
      "d490c237adb64cb1b98c977247e05cdb",
      "23b6518bba0c4a33af394bfef466e71a",
      "1dcd67c526fe4fbdb36de8b576c6207e",
      "de3b4a9f7d334a86a078bb5a2d0c95bd",
      "8eda707fd6ae4967859587e3761249cf",
      "337d271ba54849bc850b97d92afad4e6",
      "aeac4ed4c7f94e08ae7b832c517a9ee0",
      "93dc1d1430964c0a8b7ef52d9cb2c9ce",
      "71a2d4cef7554273ae87af8766f796da"
     ]
    },
    "id": "JKqVPWSe2uYg",
    "outputId": "502f46a3-9aa5-4122-9d09-845e22c3f8de"
   },
   "outputs": [],
   "source": [
    "data = generate_data_ex4b()\n",
    "print(\"The dataset contains\", data.shape[0], \"examples.\")\n",
    "HTML(data[[\"A\", \"Y\", \"Z_img\"]].head(10).to_html(\n",
    "    formatters={'Z_img': lambda im: f'<img src=\"data:image/jpeg;base64,{im}\" width=\"100\">'}, \n",
    "    escape=False, index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHhtH-4pXjLs"
   },
   "source": [
    "Next, we will train a convolutional neural network to estimate the propensity score. Concretely, we will use the picture of each invidual as input to the network and train it to predict if they received the treatment or not.\n",
    "\n",
    "<br />\n",
    "\n",
    "🟢 Let's start by loading some modules and converting the data into a PyTorch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8p6Sp9v06NhY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split the data into a training and testing set\n",
    "# Training set: will be used to fit a propensity score model\n",
    "# Testing set: will be used to estimate the treatment effect\n",
    "train_data = data.iloc[: data.shape[0] // 2]\n",
    "test_data = data.iloc[data.shape[0] // 2 : ]\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "                    torch.tensor(np.vstack([np.expand_dims(z, axis=0) for z in train_data.Z])), \n",
    "                    torch.tensor(train_data.A.values)\n",
    "                )\n",
    "       \n",
    "# Further split the training set to produce a validation set\n",
    "# that will be used to assess overfitting.\n",
    "train_size = int(len(dataset) * 0.66)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO60xfO46TLP"
   },
   "source": [
    "🟢 Next, we will define a very basic convolutional neural network, with layers that match the shape of our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOs9SO8A43xe"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic convolutional neural network\n",
    "\n",
    "    This code was taken from:\n",
    "    https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-J9n-dr6nNx"
   },
   "source": [
    "🟢 Next, we define the training loop for our model. Run the following code to train the propensity score estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IO_QkTg26nfl",
    "outputId": "e6719787-597d-41bc-9361-d6a92ac5a53d"
   },
   "outputs": [],
   "source": [
    "def evaluate_on_set(net, set, set_name):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset\n",
    "\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(set, batch_size=32, shuffle=True, num_workers=2)\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    steps = 0\n",
    "    for j, batch in enumerate(loader):\n",
    "        steps += 1\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "        accuracy += ((torch.softmax(outputs, dim=-1)[:, 1] > 0.5).int() == labels).sum() / labels.shape[0]\n",
    "    loss /= steps\n",
    "    accuracy /= steps\n",
    "    print(f\"Evaluation ({set_name}) -- loss: {loss}  accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "# Define the model, optimizer, etc.\n",
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    # Once per epoch, we calculate metrics on the whole training and validation set\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    evaluate_on_set(net, train_set, \"train\")\n",
    "    evaluate_on_set(net, val_set, \"valid\")\n",
    "    print()\n",
    "\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.float().to(device)  # Z (image)\n",
    "        labels = labels.to(device)  # A (treatment)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR8UzqUeXz-W"
   },
   "source": [
    "🟢 Then, we code a simple function to evaluate the propensity score using the trained neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlPEGn-kQ4TA"
   },
   "outputs": [],
   "source": [
    "def propensity_score(net, z, a):\n",
    "    \"\"\"\n",
    "    Get propensity score from trained neural network\n",
    "\n",
    "    \"\"\"\n",
    "    # Apply softmax to network output to get treatment probabilities\n",
    "    return torch.softmax(net(torch.tensor(np.expand_dims(z, axis=0)).float().to(device)), dim=-1)[0, a].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ii2gIuGparu"
   },
   "source": [
    "Let's look at the propensity score of the individuals in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "rZOYjtImpigm",
    "outputId": "ad1785a4-339f-49d0-e720-66c44b856240"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharex=True, sharey=True)\n",
    "sns.kdeplot(data.Z.apply(lambda z: propensity_score(net, z, 1)).values, ax=ax1)\n",
    "sns.kdeplot(data.loc[data.Z_ == 0].Z.apply(lambda z: propensity_score(net, z, 1)).values, ax=ax2, color=\"green\")\n",
    "sns.kdeplot(data.loc[data.Z_ == 1].Z.apply(lambda z: propensity_score(net, z, 1)).values, ax=ax3, color=\"orange\")\n",
    "ax1.set_xlim(0, 1)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax3.set_xlim(0, 1)\n",
    "plt.suptitle(\"Empirical densities\")\n",
    "ax1.set_title(\"$P(A = 1)$\")\n",
    "ax2.set_title(\"$P(A = 1 | Z=cat)$\")\n",
    "ax3.set_title(\"$P(A = 1 | Z=dog)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgXSCYPZs44E"
   },
   "source": [
    "Interesting, there are two modes in the distribution of estimated propensity scores and they seem to correspond to cats and dogs! So catness and dogness must have been a source of confounding here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNz0C_sgX4Qw"
   },
   "source": [
    "🟢 Finally, we implement an inverse probability weighting estimator (as described in Exercise 4) to estimate the ATE using our neural-network-based propensity score estimator. Take a look at the code and the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "CU3548lTZ8SG",
    "outputId": "7f48b3c5-df26-4c46-b202-fe7a98f5d85c"
   },
   "outputs": [],
   "source": [
    "def ate_propensity_nn(data):\n",
    "\n",
    "    # Get propensity score P(A = 1 | Z) for all individuals\n",
    "    propensity_scores = data.Z.apply(lambda z: propensity_score(net, z, 1)).values\n",
    "\n",
    "    # The danger with propensity scores, especially when using models to estimate it,\n",
    "    # is that you might get some really small values (due to modeling imperfections).\n",
    "    # When this happens, it can cause the reweighting to produce huge values that will\n",
    "    # dominate the estimated value. To avoid this, we can use clipping. Here, clipping\n",
    "    # is disabled since we use the 0% and 100% percentiles as bounds.\n",
    "    propensity_scores = np.clip(propensity_scores, \n",
    "                                np.percentile(propensity_scores, 0), \n",
    "                                np.percentile(propensity_scores, 100)\n",
    "                            )\n",
    "\n",
    "    # Estimate E[Y | do(A = 1)]\n",
    "    y_do_a1 = (1 / data.shape[0]) * (data.loc[data.A == 1].Y.values / propensity_scores[data.A == 1]).sum()\n",
    "\n",
    "    # Estimate E[Y | do(A = 0)]\n",
    "    y_do_a0 = (1 / data.shape[0]) * (data.loc[data.A == 0].Y.values / (1 - propensity_scores[data.A == 0])).sum()\n",
    "\n",
    "    return y_do_a1 - y_do_a0\n",
    "\n",
    "\n",
    "def randomized_ate():\n",
    "    \"\"\"\n",
    "    This function provides a randomization-based oracle ATE for comparison to the IPW estimator.\n",
    "\n",
    "    \"\"\"\n",
    "    data = generate_data_ex4b(randomize=True)\n",
    "    return data.loc[data.A == 1].Y.mean() - data.loc[data.A == 0].Y.mean()\n",
    "\n",
    "\n",
    "ate = {\"randomization\": randomized_ate(),\n",
    "       \"conditionals\": data.loc[data.A == 1].Y.mean() - data.loc[data.A == 0].Y.mean(),\n",
    "       \"trained model\": ate_propensity_nn(test_data)}\n",
    "\n",
    "pprint_ates(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ant7e4fjqSfO"
   },
   "source": [
    "You should get an estimate that is somewhat close to the ground truth. In practice, since we are using modeling, we might not get a perfect result. Getting a number that is at least of the same sign as the ground truth (obtained via randomization) is already a nice achievement. If you get something that is really far from the ground truth, re-train the neural network and see if a different initialization gives a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "broad-processing"
   },
   "source": [
    "## Exercise 5: Causal Structure Learning (10 min)\n",
    "\n",
    "This is the question of concern now. \n",
    "\n",
    "We have seen why causality is needed in machine learning by observing a hopeless situation, where regular machine learning fails.\n",
    "\n",
    "We have seen all key elements of reasoning, and how they connect to what we already know from regular probability theory.\n",
    "\n",
    "But now that we know what an SCM is, how do we get it, or *how do we even get only the graph?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mechanical-dispatch"
   },
   "source": [
    "In the following, we will use show the application of two different methods from the causal discovery literature.\n",
    "\n",
    "* The first algorithm comes from the family of constraint-based methods. The first ever proven both sound and complete causal discovery algorithm, **Fast Causal Inference** (Spirtes et al., 2000), FCI for short. FCI works both fast and gives guarantees, however, comes at the lack of being able to discern graphs which share a notion of equivalence (i.e., we usually won't get the exact, unique graph of our problem at hand).\n",
    "\n",
    "  The algorithm exploits *conditional independences* found in the data to construct the graph by adhering to what is known as $d$-separation, a graphical criertion which establishes an equivalence to the former by the Markov condition and faithfulness, formally $A \\underset{P}{\\perp} B | C \\iff A \\underset{G}{\\perp} B | C$ where an independence in $P$ is equivalent to a separation in graph $G$. \n",
    "\n",
    "Be assured, we need not go in the details here, but we will simply explore our original biology example using FCI.\n",
    "\n",
    "* The second algorithm is a more recent one that disconnects from causal guarantees (i.e., is really concerned with evaluating sensible structures) but is very effective, NOTEARS (Zheng et al. 2018), NT for short. Unfortunately, the algorithm scales cubically in the number of dimensions $d, \\mathcal{O}(d^3)$ rendering it restrictive but it offers a complete continuous-optimization characterization of the structure learning problems.\n",
    "\n",
    "  The algorithm optimizes the mean-squared error (MSE) over DAGs, where a DAG is guaranteed via convergence to an acylicity constraint.\n",
    "\n",
    "Again, be assured, we need not go in the details here, but we will also explore our original biology example using NT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simple-taste"
   },
   "outputs": [],
   "source": [
    "def graph_induction(D, method):\n",
    "\n",
    "    if method == \"notears\":\n",
    "        G_pred = notears_linear(D, lambda1 = .1, loss_type = 'l2')\n",
    "    elif method == \"fci\":\n",
    "        G_pred = fci(D, verbose=False)\n",
    "        G_pred = G_pred[0].graph\n",
    "\n",
    "    return G_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "racial-clerk"
   },
   "source": [
    "Let's load our data again. Note that we are going to only look at our observational data i.e., the data which originally did not help us discern the causal system, since we did not ask for a causal model. That is, we will try to find the true causal graph of $B\\leftarrow C\\rightarrow P$ just from data points. This time around we will asumme that we observe the confounder $C$ (but we don't know yet whether it is actually a confounder, since of course, we want to know the graph now).\n",
    "\n",
    "We will consider an alternate version of our data set in which we know the underlying functions to be non-linear, this will render our causal structure *identifiable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single-referral",
    "outputId": "1b345a20-4f43-4234-cb25-ee5efe904c45"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset-Example-Phenotype-and-Genes-Nonlinear.csv\")\n",
    "\n",
    "df = df.loc[:,[\"confounder\", \"gene_B\", \"phenotype\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f0418073f95e4ae7bffde0ecb5279cc3"
     ]
    },
    "id": "threatened-cooperation",
    "outputId": "c7afbd5b-86be-4a7f-ceb8-14adbf2bbff9"
   },
   "outputs": [],
   "source": [
    "D = df.to_numpy()\n",
    "\n",
    "G_pred_fci = graph_induction(D, method=\"fci\")\n",
    "G_pred_nt = graph_induction(D, method=\"notears\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acute-psychiatry",
    "outputId": "d6625a85-c43d-44fc-e937-478e3640b216"
   },
   "outputs": [],
   "source": [
    "dims = len(df.columns)\n",
    "dict_visualization = {\n",
    "    \"Fast Causal Inference Algorithm\": G_pred_fci,\n",
    "    \"NOTEARS Algorithm\": G_pred_nt,\n",
    "}\n",
    "\n",
    "sharex = sharey = True\n",
    "experiment_description = f'''\n",
    "Inferring the Causal Graph of our Biology Data Set from purely Observational Data\\n\n",
    "Top Row shows FCI/NT predicted graph as an adjacency matrix,\n",
    "Bottom Row shows the actual graph\n",
    "'''\n",
    "suptitle = f'{experiment_description}'\n",
    "\n",
    "commonlabel = list(df.columns)\n",
    "utils.plot_all_individual(list(dict_visualization.values()),\n",
    "                    list(dict_visualization.keys()),\n",
    "                    suptitle=suptitle,\n",
    "                    alt_form=(1,2),\n",
    "                    alt_size=(10,6),\n",
    "                    sharex=sharex,\n",
    "                    sharey=sharey,\n",
    "                    commonlabel=commonlabel)\n",
    "\n",
    "dict_cyc_vis = dict_visualization\n",
    "utils.plot_digraphs_and_cycles(list(dict_cyc_vis.values()), list(dict_cyc_vis.keys()), commonlabel,\n",
    "                    alt_size=(13,5), arrowsize=14, font_size=10, node_size=250, no_cycles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rocky-fishing"
   },
   "source": [
    "As expected, both algorithms identify the structure, but FCI only up to Markov Equivalence whereas NOTEARS manages to find the exact causal graph (however, there is no guarantee --- beginner's luck you might call it here).\n",
    "\n",
    "**To conclude we can use the available data in different, creative ways to find out about its origin, that is, the data generating SCM's causal graph - amazing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MF90WvtcaxWi"
   },
   "source": [
    "## Additional Resources (some additions to the Slides)\n",
    "\n",
    "\n",
    "**Books:**\n",
    "\n",
    "The concepts that we explored are well-defined in the following books:\n",
    "* Pearl, Judea. Causality. Cambridge university press, 2009.\n",
    "* Peters, Jonas, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. [PDF](https://library.oapen.org/bitstream/handle/20.500.12657/26040/11283.pdf?sequence=1&isAllowed=y)\n",
    "* Hernán MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC. [PDF](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)\n",
    "\n",
    "For a perspective on the historical development of the ideas and a compressed summary of the most important papers from the 1980s till today, please consider:\n",
    "* Geffner, Hector et al. Probabilistic and Causal Inference: The Works of Judea Pearl. [ACM](https://dl.acm.org/doi/book/10.1145/3501714)\n",
    "\n",
    "For an intuitive, general audience introduction consider:\n",
    "* Pearl, Judea. The Book of Why. [WHY](http://bayes.cs.ucla.edu/WHY/)\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "**Other tutorials:**\n",
    "\n",
    "* [David Sontag](https://people.csail.mit.edu/dsontag/)'s video lectures: [part 1](https://www.youtube.com/watch?v=gRkUhg9Wb-I) [part 2](https://www.youtube.com/watch?v=g5v-NvNoJQQ)\n",
    "* [Jason Roy's Coursera Course on Causal Inference](https://www.coursera.org/learn/crash-course-in-causality)\n",
    "* [Brady Neal's Online Causal Inference Course](https://www.bradyneal.com/causal-inference-course)\n",
    "* [Elias Bareinboim Seminar Lecture](https://www.youtube.com/watch?v=RzhcZpPt_Hs)\n",
    "\n",
    " <br />\n",
    " \n",
    "**Python packages for causal inference:**\n",
    "\n",
    "* [DoWhy](https://github.com/py-why/dowhy) and [EconML](https://github.com/microsoft/EconML): estimate causal effects via various adjustment methods (e.g., back-door)\n",
    "* [CausalLearn](https://github.com/cmu-phil/causal-learn): causal discovery algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djAACxO1uir6"
   },
   "source": [
    "## ⭐⭐⭐ To Learn More ⭐⭐⭐ \n",
    "\n",
    "Please check out the weekly Causality Discussion Group at [discuss.causality.link](discuss.causality.link)\n",
    "\n",
    "<img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/CDG-logo.png\" width=\"300\" />\n",
    "\n",
    "Please consider participating at the NeurIPS 2022 workshop - neuro Causal & Symbolic AI ([ncsi.cause-lab.net](ncsi.cause-lab.net)).\n",
    "\n",
    "<a href=\"https://ncsi.cause-lab.net/\" target=\"_blank\"><img src=\"https://github.com/aldro61/eeml_causal_tutorial/raw/main/ncsi.png\" width=\"800\" /></a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UefDxwkUKzbo"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "117353bf99e44b89b1c2556a7018ef8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de3b4a9f7d334a86a078bb5a2d0c95bd",
      "placeholder": "​",
      "style": "IPY_MODEL_8eda707fd6ae4967859587e3761249cf",
      "value": ""
     }
    },
    "1dcd67c526fe4fbdb36de8b576c6207e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b6518bba0c4a33af394bfef466e71a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93dc1d1430964c0a8b7ef52d9cb2c9ce",
      "placeholder": "​",
      "style": "IPY_MODEL_71a2d4cef7554273ae87af8766f796da",
      "value": " 170499072/? [00:01&lt;00:00, 91224349.64it/s]"
     }
    },
    "337d271ba54849bc850b97d92afad4e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71a2d4cef7554273ae87af8766f796da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8eda707fd6ae4967859587e3761249cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9132aab26a4d4f14bbb82a6a1f322291": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_117353bf99e44b89b1c2556a7018ef8a",
       "IPY_MODEL_d490c237adb64cb1b98c977247e05cdb",
       "IPY_MODEL_23b6518bba0c4a33af394bfef466e71a"
      ],
      "layout": "IPY_MODEL_1dcd67c526fe4fbdb36de8b576c6207e"
     }
    },
    "93dc1d1430964c0a8b7ef52d9cb2c9ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeac4ed4c7f94e08ae7b832c517a9ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d490c237adb64cb1b98c977247e05cdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337d271ba54849bc850b97d92afad4e6",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aeac4ed4c7f94e08ae7b832c517a9ee0",
      "value": 170498071
     }
    },
    "de3b4a9f7d334a86a078bb5a2d0c95bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
